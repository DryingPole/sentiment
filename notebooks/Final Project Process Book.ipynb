{
 "metadata": {
  "name": "",
  "signature": "sha256:c295f6361cb43fe2beecf9e6556a0174fc60d63cb20b5636c15fdad8408e93bc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# special IPython command to prepare the notebook for matplotlib\n",
      "%matplotlib inline \n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "\n",
      "# load required modules\n",
      "import requests \n",
      "import StringIO\n",
      "import zipfile\n",
      "import numpy as np\n",
      "import pandas as pd \n",
      "import matplotlib.pyplot as plt \n",
      "import datetime as dt \n",
      "import random\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# load custom modules\n",
      "import imp\n",
      "source_path = '../sentiment/'\n",
      "core = imp.load_source('core', source_path + 'core.py')\n",
      "bow = imp.load_source('bow', source_path + 'bow.py')\n",
      "bayes = imp.load_source('bayes', source_path + 'bayes.py')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our first model will be based on the Bag Of Words approach. One challenge particular to our project that will likely be present in all of the models we develop, is that we are trying to classify reviews on an integer scale from 0 to 4, not just as \"positive\" or \"negative\".\n",
      "Since corpora are typically graded binarily as described above and also depending on context, we will have to develop our own dictionary.\n",
      "Also, since our scale is more granular, a simple BoW approach whereby we vectorize phrases based on word count will not work.\n",
      "\n",
      "Our first try with this method will consist of building a dictionary of single words with their associated rating from 0 to 4.\n",
      "We will use both the training dataset provided by Kaggle as well as the training dataset used by the Stanford team, also from Rotten Tomatoes reviews but different ones.\n",
      "When confronted to a new review, our model will add up the values of the grades of the words present in its dictionary, and compute the mean (try geometric mean?) of the total value for that phrase.\n",
      "\n",
      "One problem we will run into is that some phrases in the test dataset or in any review not present in our two training datasets might contain words not in our dictionary, or even not have any words present in our dictionary.\n",
      "For now, we will assign a random rating to phrases for which we have no rated words whatsoever, and just compute the mean of the words we have in our dictionary for sentences in which we don't have all the words in our dictionary.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Step 1**: Build the dictionary.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use 'load_reviews' to load the data set into a data frame. This will \"normalize\" the \n",
      "# headings and make all phrases lower case. \n",
      "phrases = core.load_reviews('../resources/train.tsv')\n",
      "phrases.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>phraseid</th>\n",
        "      <th>sentenceid</th>\n",
        "      <th>phrase</th>\n",
        "      <th>sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> a series of escapades demonstrating the adage ...</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> a series of escapades demonstrating the adage ...</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                          a series</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 4</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                                 a</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 5</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                            series</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "   phraseid  sentenceid                                             phrase  \\\n",
        "0         1           1  a series of escapades demonstrating the adage ...   \n",
        "1         2           1  a series of escapades demonstrating the adage ...   \n",
        "2         3           1                                           a series   \n",
        "3         4           1                                                  a   \n",
        "4         5           1                                             series   \n",
        "\n",
        "   sentiment  \n",
        "0          1  \n",
        "1          2  \n",
        "2          2  \n",
        "3          2  \n",
        "4          2  "
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use the one-word phrases as a training set for the BoW model. Split the data set into training data - one-word phrases -\n",
      "# and test data - all other phrases\n",
      "single_word = r'\\A[\\w-]+\\Z'\n",
      "\n",
      "train = phrases[phrases.phrase.str.match(single_word)][['phrase', 'sentiment']]\n",
      "X_train, Y_train = train.phrase.tolist(), train.sentiment.tolist()\n",
      "\n",
      "test = phrases[map(lambda x: not x, phrases.phrase.str.match(single_word))][['phrase', 'sentiment']]\n",
      "X_test, Y_test = test.phrase, test.sentiment\n",
      "\n",
      "print \"Training Set:\"\n",
      "print train.head(5)\n",
      "print \"\\nTest Set:\"\n",
      "print test.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training Set:\n",
        "           phrase  sentiment\n",
        "3               a          2\n",
        "4          series          2\n",
        "6              of          2\n",
        "8       escapades          2\n",
        "11  demonstrating          2\n",
        "\n",
        "Test Set:\n",
        "                                              phrase  sentiment\n",
        "0  a series of escapades demonstrating the adage ...          1\n",
        "1  a series of escapades demonstrating the adage ...          2\n",
        "2                                           a series          2\n",
        "5  of escapades demonstrating the adage that what...          2\n",
        "7  escapades demonstrating the adage that what is...          2\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bow_model = bow.BagOfWordsModel()\n",
      "bow_model.fit(X_train, Y_train)\n",
      "print bow_model._not_found\n",
      "\n",
      "Y = bow_model.predict(X_test)\n",
      "\n",
      "res_df = pd.DataFrame(data=np.transpose([Y, Y_test.tolist()]), columns=[\"predicted\", \"expected\"])\n",
      "print res_df\n",
      "res_stats = res_df.groupby([\"predicted\", \"expected\"])\n",
      "# print res_stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "        predicted  expected\n",
        "0               2         1\n",
        "1               2         2\n",
        "2               2         2\n",
        "3               2         2\n",
        "4               2         2\n",
        "5               2         2\n",
        "6               2         2\n",
        "7               2         2\n",
        "8               2         2\n",
        "9               2         2\n",
        "10              2         2\n",
        "11              2         3\n",
        "12              2         2\n",
        "13              2         2\n",
        "14              2         2\n",
        "15              2         2\n",
        "16              2         2\n",
        "17              2         2\n",
        "18              2         2\n",
        "19              2         1\n",
        "20              2         2\n",
        "21              2         2\n",
        "22              2         2\n",
        "23              2         2\n",
        "24              2         2\n",
        "25              2         2\n",
        "26              2         2\n",
        "27              2         2\n",
        "28              2         1\n",
        "29              2         1\n",
        "...           ...       ...\n",
        "139659          2         2\n",
        "139660          1         1\n",
        "139661          1         1\n",
        "139662          1         2\n",
        "139663          1         1\n",
        "139664          1         1\n",
        "139665          2         1\n",
        "139666          2         1\n",
        "139667          2         2\n",
        "139668          2         1\n",
        "139669          2         2\n",
        "139670          1         2\n",
        "139671          2         2\n",
        "139672          2         2\n",
        "139673          2         2\n",
        "139674          2         2\n",
        "139675          2         3\n",
        "139676          2         2\n",
        "139677          2         2\n",
        "139678          2         1\n",
        "139679          2         2\n",
        "139680          2         2\n",
        "139681          2         2\n",
        "139682          2         1\n",
        "139683          2         1\n",
        "139684          1         2\n",
        "139685          1         2\n",
        "139686          2         2\n",
        "139687          1         1\n",
        "139688          2         3\n",
        "\n",
        "[139689 rows x 2 columns]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Step 2**: Write a function that takes a dataframe of reviews and gives them a sentiment rating.\n",
      "Also write a function to check the ratings to in order to test our algorithm on the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Function\n",
      "--------\n",
      "Rate reviews using Bag of Words model\n",
      "\n",
      "Returns\n",
      "-------\n",
      "A new dataframe with a \"Sentiment\" column containing the predicted sentiments for each phrase\n",
      "\n",
      "'''\n",
      "def bow_rating(reviews):\n",
      "    df = reviews.copy()\n",
      "    sentiments = []\n",
      "    randoms = 0 # keep track of randomly rated phrases\n",
      "    not_found = []\n",
      "    for p in reviews['Phrase']:    \n",
      "        accum = 0 \n",
      "        words_in_dic = 0 \n",
      "        mean = 0\n",
      "        split = p.split() # split phrase into words\n",
      "        for s in split:\n",
      "            if s in dic:\n",
      "                words_in_dic += 1\n",
      "                accum += dic[s]\n",
      "        # check for case where no words were found in our dictionary\n",
      "        if words_in_dic == 0:\n",
      "            mean = random.randint(0, 4) # generate random sentiment from 0 to 4\n",
      "            randoms += 1\n",
      "            not_found.append(p) \n",
      "        else:        \n",
      "            mean = round(accum / words_in_dic)\n",
      "\n",
      "        sentiments.append(mean)\n",
      "\n",
      "    # add 'Sentiment' column containing predicted sentiments to new dataframe\n",
      "    df['Sentiment'] = sentiments \n",
      "    print \"randomly rated:\", randoms\n",
      "\n",
      "    return df, not_found\n",
      "\n",
      "\n",
      "'''\n",
      "Function\n",
      "--------\n",
      "Check predicted ratings accuracy when testing on our training set\n",
      "\n",
      "Returns\n",
      "-------\n",
      "An accuracy percentage\n",
      "\n",
      "'''\n",
      "\n",
      "def check_ratings(predictions, test_set):\n",
      "    hits = 0.\n",
      "    i = len(predictions)\n",
      "    for p in range(i):\n",
      "        if predictions['Sentiment'][p] == test_set['Sentiment'][p]:\n",
      "            hits += 1\n",
      "    return (hits / i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test bow_rating function on training set\n",
      "# create new df without Sentiment column of training set\n",
      "trainset_test = train_phrases.copy()\n",
      "trainset_test = trainset_test.drop(['Sentiment'], axis = 1)\n",
      "\n",
      "results, not_found = bow_rating(trainset_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "randomly rated: 183\n"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot something here, for ex distribution of ratings\n",
      "collections.Counter(results.Sentiment)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 160,
       "text": [
        "Counter({2.0: 119685, 1.0: 30863, 3.0: 4765, 0.0: 398, 4.0: 349})"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy = check_ratings(results, train_phrases)\n",
      "accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 158,
       "text": [
        "0.5487632961681405"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "not_found"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 159,
       "text": [
        "[',',\n",
        " '.',\n",
        " \"'s\",\n",
        " \"n't\",\n",
        " 'mr.',\n",
        " \"'d\",\n",
        " '``',\n",
        " \"''\",\n",
        " \"'\",\n",
        " '...',\n",
        " \"'re\",\n",
        " '`',\n",
        " \"'m\",\n",
        " '!',\n",
        " \"o'fallon\",\n",
        " ':',\n",
        " ' ',\n",
        " 'writer\\\\/director',\n",
        " 'u.n.',\n",
        " 'd.',\n",
        " 's.c.',\n",
        " '?',\n",
        " \"'60s\",\n",
        " ';',\n",
        " \"'ve\",\n",
        " '=',\n",
        " \"'n\",\n",
        " \"'ll\",\n",
        " 'e.t.',\n",
        " '21\\\\/2',\n",
        " '$',\n",
        " 'r.',\n",
        " 'groupie\\\\/scholar',\n",
        " 'mother\\\\/daughter',\n",
        " \"'n'\",\n",
        " 'd.w.',\n",
        " '$ 1.8',\n",
        " '1.8',\n",
        " 'ms.',\n",
        " '?!?',\n",
        " '&',\n",
        " \"'50s\",\n",
        " \"'s `\",\n",
        " 'star\\\\/producer',\n",
        " 'coming-of-age\\\\/coming-out',\n",
        " \"'70s\",\n",
        " \"'re `\",\n",
        " ', inc. ,',\n",
        " 'inc. ,',\n",
        " 'inc.',\n",
        " 'etc.',\n",
        " 'e.t',\n",
        " 'c.h.o.',\n",
        " 'j.',\n",
        " 'hopkins\\\\/rock',\n",
        " 'm.',\n",
        " '\\\\*\\\\*',\n",
        " 'no. .',\n",
        " 'no.',\n",
        " 'i.q.',\n",
        " 'thriller\\\\/horror',\n",
        " 'israeli\\\\/palestinian',\n",
        " '3\\\\/4th',\n",
        " \"ol'\",\n",
        " \"n.m. '\",\n",
        " 'n.m.',\n",
        " 'b.s.',\n",
        " 'p.o.v.',\n",
        " '2\\\\/3',\n",
        " '\\\\/',\n",
        " 'o.k. ,',\n",
        " 'o.k.',\n",
        " 'vs.',\n",
        " \"jr. 's\",\n",
        " 'jr.',\n",
        " '\\\\*',\n",
        " 'actor\\\\/director',\n",
        " 'direct-to-video\\\\/dvd',\n",
        " '#',\n",
        " \"c'mon !\",\n",
        " \"c'mon\",\n",
        " 'k.',\n",
        " \"'em\",\n",
        " 'm.i.t.',\n",
        " 'gangster\\\\/crime',\n",
        " 'action\\\\/thriller',\n",
        " '\\\\*\\\\*\\\\*',\n",
        " 'j.k.',\n",
        " 'a.',\n",
        " 'co-writer\\\\/director',\n",
        " '1.2',\n",
        " 'bros.',\n",
        " 'marine\\\\/legal',\n",
        " 'comedy\\\\/drama',\n",
        " \"bettany\\\\/mcdowell 's\",\n",
        " 'bettany\\\\/mcdowell',\n",
        " 'j.r.r.',\n",
        " 'p.c.',\n",
        " 'l.',\n",
        " 'p.t.',\n",
        " 'wash.',\n",
        " \"'s ...\",\n",
        " 'hiv\\\\/aids',\n",
        " 'h.g.',\n",
        " \"d'etre\",\n",
        " \"'s ``\",\n",
        " 'c.i.',\n",
        " '1\\\\/2',\n",
        " 's&m',\n",
        " 'x.',\n",
        " ': no. .',\n",
        " '\\\\*\\\\*\\\\*\\\\*',\n",
        " 'bollywood\\\\/hollywood',\n",
        " \"'80s\",\n",
        " 'dr.',\n",
        " 'p.o.w.',\n",
        " 'a.s.',\n",
        " 'big-budget\\\\/all-star',\n",
        " 'feardotcom.com',\n",
        " \"!? '\",\n",
        " '!?',\n",
        " 'i.e. ,',\n",
        " 'i.e.',\n",
        " 'l.a.',\n",
        " \"l.a. 's\",\n",
        " 'social\\\\/economic\\\\/urban',\n",
        " '4\\\\/5ths',\n",
        " 's.',\n",
        " 'u.',\n",
        " \"'s ,\",\n",
        " 'q.',\n",
        " 'and\\\\/or',\n",
        " '+',\n",
        " 'drama\\\\/character',\n",
        " 'horror\\\\/action',\n",
        " 'nature\\\\/nurture',\n",
        " 'a.c.',\n",
        " 'w.',\n",
        " 'al.',\n",
        " \"'90s\",\n",
        " 'director\\\\/co-writer',\n",
        " \"'53\",\n",
        " '9\\\\/11',\n",
        " 'writer\\\\/directors',\n",
        " 'c.',\n",
        " 'g.',\n",
        " 'disappearing\\\\/reappearing',\n",
        " 't.',\n",
        " 'd.j.',\n",
        " 'action\\\\/comedy',\n",
        " 's\\\\/m',\n",
        " 'romantic\\\\/comedy',\n",
        " 'a. . .',\n",
        " 'a. .',\n",
        " \"'til\",\n",
        " 'action-thriller\\\\/dark',\n",
        " 'damon\\\\/bourne',\n",
        " ', inc.',\n",
        " 'sept.',\n",
        " 'r&b',\n",
        " 'reel\\\\/real',\n",
        " 'venice\\\\/venice',\n",
        " 'u.s.',\n",
        " \"'30s\",\n",
        " \"'40s\",\n",
        " 'drama\\\\/action',\n",
        " 'brothers\\\\/abrahams',\n",
        " 'video\\\\/dvd',\n",
        " 'mrs.',\n",
        " \"'s '\",\n",
        " '20,000',\n",
        " 'co.',\n",
        " 'f.',\n",
        " '10,000',\n",
        " 'a.e.w.',\n",
        " 'o.',\n",
        " 'v.s.',\n",
        " 'juliet\\\\/west',\n",
        " '2,500',\n",
        " 'monster\\\\/science',\n",
        " 'singer\\\\/composer',\n",
        " 'r&d',\n",
        " '24\\\\/7']"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "also we can see that only 806 reviews are randomly rated by our algorithm because no words in the phrase were found\n",
      "in our dictionary, which accounts for ~ 0.5% of all our ratings\n",
      "\n",
      "ACTUALLY IT\"S VERY WEIRD THAT THERE ARE RANDOMLY RATED PHRASES WHEN RUNNING OUR DICTIONARY ON THE TRAINING SET SINCE \n",
      "ALL THE WORDS ARE SUPPOSED TO BE THERE!!!\n",
      "\n",
      "When we print out all the phrases for which no words were found in our dictionary, we see that some of them should be in there, but 'dangerous' for example, was rated in the training set with a capital 'D', 'average' is never rated, etc...Maybe by homogenizing the capitalization to lower-case, we can bring the number down of not-found phrases.\n",
      "\n",
      "After turning everything to lower-case in the original training set and basing our dictionary on that, the number of phrases with no words found at all was reduced from 806 to 183.\n",
      "\n",
      "**Now let's try with test set, submit to Kaggle. Then try tweaking, stop list, synonyms, etc...**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load test set\n",
      "test_phrases = pd.read_table('https://sites.google.com/site/sentananianlucas/data-and-other-material/test.tsv?attredirects=0&d=1')\n",
      "# make everything lower-case\n",
      "test_phrases.Phrase = test_phrases.Phrase.str.lower()\n",
      "test_phrases.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>PhraseId</th>\n",
        "      <th>SentenceId</th>\n",
        "      <th>Phrase</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 156061</td>\n",
        "      <td> 8545</td>\n",
        "      <td> an intermittently pleasing but mostly routine ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 156062</td>\n",
        "      <td> 8545</td>\n",
        "      <td> an intermittently pleasing but mostly routine ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 156063</td>\n",
        "      <td> 8545</td>\n",
        "      <td>                                                an</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 156064</td>\n",
        "      <td> 8545</td>\n",
        "      <td> intermittently pleasing but mostly routine effort</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 156065</td>\n",
        "      <td> 8545</td>\n",
        "      <td>        intermittently pleasing but mostly routine</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 164,
       "text": [
        "   PhraseId  SentenceId                                             Phrase\n",
        "0    156061        8545  an intermittently pleasing but mostly routine ...\n",
        "1    156062        8545  an intermittently pleasing but mostly routine ...\n",
        "2    156063        8545                                                 an\n",
        "3    156064        8545  intermittently pleasing but mostly routine effort\n",
        "4    156065        8545         intermittently pleasing but mostly routine"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ratings, rand_rated = bow_rating(test_phrases)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "randomly rated: 3329\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare the DF for submission on Kaggle\n",
      "sub = ratings.drop((['SentenceId', 'Phrase']), axis = 1)\n",
      "# turn Sentiment values into ints\n",
      "sub.Sentiment = sub.Sentiment.astype(int)\n",
      "# turn into CSV file\n",
      "sub.to_csv('/Users/barbaramaclaurin/Desktop/Final Project CS109/Submissions/sub1', index = False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 192
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**TRY:**\n",
      "Use stop words list to keep only adjectives because if not, words like \"the\", rated 2, would lower the average of a phrase containing \"amazing\", etc...although maybe not a problem since it'll happen randomly.\n",
      "\n",
      "Make a dictionary where each synonym of the graded words we have is graded the same?\n",
      "Use negation? How?\n",
      "\n",
      "Maybe best model is stop words list and using negation, however that's a problem because we have to filter the phrases with the stop words list, so we'll lose \"not\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 193
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}